{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cpu\n",
      "✅ Train: 2975 | Val: 500\n",
      "\n",
      "🔁 Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔧 Training:   0%|                                                                            | 0/1488 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ✅ 패키지\n",
    "# ---------------------------------------------------------\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ✅ 데이터셋\n",
    "# ---------------------------------------------------------\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, resize=(512, 512), mode='train'):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*\", \"*.png\")))\n",
    "        self.mask_paths  = [p.replace(\"images\", \"gtFine\").replace(\"_leftImg8bit\", \"_gtFine_labelIds\") for p in self.image_paths]\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(*resize),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.3),\n",
    "                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(*resize),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx])).astype(np.int64)\n",
    "\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = torch.from_numpy(augmented['mask']).long()  # ✅ FIXED: long 타입 변환\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ✅ 경로 및 데이터로더\n",
    "# ---------------------------------------------------------\n",
    "train_image_dir = r\"C:\\Users\\ghwns\\HJ_git\\CV-Projects\\urban-scene-segmentation\\Dataset\\Cityspaces\\images\\train\"\n",
    "val_image_dir   = r\"C:\\Users\\ghwns\\HJ_git\\CV-Projects\\urban-scene-segmentation\\Dataset\\Cityspaces\\images\\val\"\n",
    "\n",
    "train_dataset = CityscapesDataset(train_image_dir, mode='train')\n",
    "val_dataset   = CityscapesDataset(val_image_dir, mode='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"✅ Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ✅ 모델, 손실함수, 옵티마이저\n",
    "# ---------------------------------------------------------\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=34\n",
    ").to(device)\n",
    "\n",
    "loss_dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "loss_ce   = nn.CrossEntropyLoss()\n",
    "def combo_loss(pred, target):\n",
    "    return 0.5 * loss_ce(pred, target) + 0.5 * loss_dice(pred, target)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ✅ 학습 루프\n",
    "# ---------------------------------------------------------\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(f\"\\n🔁 Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, masks in tqdm(train_loader, desc=\"🔧 Training\", leave=False):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = combo_loss(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for images, masks in tqdm(val_loader, desc=\"🔍 Validating\", leave=False):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            loss = combo_loss(outputs, masks)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(f\"✅ Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ✅ mIoU 계산\n",
    "# ---------------------------------------------------------\n",
    "def fast_hist(pred, label, num_class):\n",
    "    mask = (label >= 0) & (label < num_class)\n",
    "    return np.bincount(num_class * label[mask].astype(int) + pred[mask], minlength=num_class ** 2).reshape(num_class, num_class)\n",
    "\n",
    "def per_class_iou(hist):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
    "\n",
    "def compute_mIoU(model, dataloader, num_classes=34):\n",
    "    model.eval()\n",
    "    hist = np.zeros((num_classes, num_classes))\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"📏 Calculating mIoU\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            targets = masks.cpu().numpy()\n",
    "            for p, t in zip(preds, targets):\n",
    "                hist += fast_hist(p.flatten(), t.flatten(), num_classes)\n",
    "\n",
    "    ious = per_class_iou(hist)\n",
    "    miou = np.nanmean(ious)\n",
    "    print(f\"📊 Final mIoU: {miou:.4f}\")\n",
    "    return miou\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ✅ 실행\n",
    "# ---------------------------------------------------------\n",
    "compute_mIoU(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5341b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
