{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bb01b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3001 images belonging to 8 classes.\n",
      "Found 743 images belonging to 8 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.09551, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 29s - 1s/step - accuracy: 0.1529 - loss: 2.4313 - val_accuracy: 0.2275 - val_loss: 2.0955 - learning_rate: 3.0000e-04\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: val_loss improved from 2.09551 to 1.81135, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 40s - 2s/step - accuracy: 0.2612 - loss: 1.9911 - val_accuracy: 0.3405 - val_loss: 1.8113 - learning_rate: 3.0000e-04\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_loss improved from 1.81135 to 1.63444, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 25s - 1s/step - accuracy: 0.3622 - loss: 1.7429 - val_accuracy: 0.4051 - val_loss: 1.6344 - learning_rate: 3.0000e-04\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_loss improved from 1.63444 to 1.52237, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 18s - 745ms/step - accuracy: 0.4299 - loss: 1.5786 - val_accuracy: 0.4522 - val_loss: 1.5224 - learning_rate: 3.0000e-04\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: val_loss improved from 1.52237 to 1.44877, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 18s - 732ms/step - accuracy: 0.4705 - loss: 1.4700 - val_accuracy: 0.4818 - val_loss: 1.4488 - learning_rate: 3.0000e-04\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_loss improved from 1.44877 to 1.39759, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 28s - 1s/step - accuracy: 0.4965 - loss: 1.3897 - val_accuracy: 0.4953 - val_loss: 1.3976 - learning_rate: 3.0000e-04\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_loss improved from 1.39759 to 1.35866, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 37s - 2s/step - accuracy: 0.5175 - loss: 1.3300 - val_accuracy: 0.5074 - val_loss: 1.3587 - learning_rate: 3.0000e-04\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 8: val_loss improved from 1.35866 to 1.32997, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 26s - 1s/step - accuracy: 0.5332 - loss: 1.2819 - val_accuracy: 0.5128 - val_loss: 1.3300 - learning_rate: 3.0000e-04\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 9: val_loss improved from 1.32997 to 1.30709, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 49s - 2s/step - accuracy: 0.5418 - loss: 1.2439 - val_accuracy: 0.5262 - val_loss: 1.3071 - learning_rate: 3.0000e-04\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: val_loss improved from 1.30709 to 1.28721, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 27s - 1s/step - accuracy: 0.5591 - loss: 1.2083 - val_accuracy: 0.5289 - val_loss: 1.2872 - learning_rate: 3.0000e-04\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 11: val_loss improved from 1.28721 to 1.27212, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 36s - 1s/step - accuracy: 0.5715 - loss: 1.1781 - val_accuracy: 0.5343 - val_loss: 1.2721 - learning_rate: 3.0000e-04\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 12: val_loss improved from 1.27212 to 1.25869, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 27s - 1s/step - accuracy: 0.5801 - loss: 1.1522 - val_accuracy: 0.5370 - val_loss: 1.2587 - learning_rate: 3.0000e-04\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: val_loss improved from 1.25869 to 1.24954, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 50s - 2s/step - accuracy: 0.5905 - loss: 1.1307 - val_accuracy: 0.5370 - val_loss: 1.2495 - learning_rate: 3.0000e-04\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 14: val_loss improved from 1.24954 to 1.24097, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 47s - 2s/step - accuracy: 0.6018 - loss: 1.1060 - val_accuracy: 0.5424 - val_loss: 1.2410 - learning_rate: 3.0000e-04\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 15: val_loss improved from 1.24097 to 1.23338, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 46s - 2s/step - accuracy: 0.6098 - loss: 1.0856 - val_accuracy: 0.5397 - val_loss: 1.2334 - learning_rate: 3.0000e-04\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: val_loss improved from 1.23338 to 1.22725, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 60s - 2s/step - accuracy: 0.6151 - loss: 1.0669 - val_accuracy: 0.5437 - val_loss: 1.2272 - learning_rate: 3.0000e-04\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 17: val_loss improved from 1.22725 to 1.22192, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 73s - 3s/step - accuracy: 0.6205 - loss: 1.0503 - val_accuracy: 0.5424 - val_loss: 1.2219 - learning_rate: 3.0000e-04\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: val_loss improved from 1.22192 to 1.21654, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 62s - 3s/step - accuracy: 0.6295 - loss: 1.0335 - val_accuracy: 0.5478 - val_loss: 1.2165 - learning_rate: 3.0000e-04\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: val_loss improved from 1.21654 to 1.21238, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 50s - 2s/step - accuracy: 0.6338 - loss: 1.0211 - val_accuracy: 0.5424 - val_loss: 1.2124 - learning_rate: 3.0000e-04\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: val_loss improved from 1.21238 to 1.20569, saving model to mobilenetv2_adamtune_best.keras\n",
      "24/24 - 60s - 3s/step - accuracy: 0.6415 - loss: 1.0051 - val_accuracy: 0.5532 - val_loss: 1.2057 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    }
   ],
   "source": [
    "# üìÖ 2025/06/06 - MobileNetV2 + Adam ÌäúÎãù (RGB Input, ImageNet weights)\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú (RGB Ïù¥ÎØ∏ÏßÄÍ∞Ä class Ìè¥Îçî ÏïàÏóê ÏûàÏùå)\n",
    "data_dir = r\"C:\\Users\\ghwns\\HJ_git\\CV-Projects\\real-time-daily-activity-recognizer\\images\"\n",
    "\n",
    "# ImageDataGenerator ÏÑ§Ï†ï (ImageNet Ï†ÑÏ≤òÎ¶¨ + Í≤ÄÏ¶ù Î∂ÑÌï†)\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(128, 128),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=128,\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(128, 128),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=128,\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# MobileNetV2 Í∏∞Î∞ò Ï†ÑÏù¥ÌïôÏäµ Î™®Îç∏ Ï†ïÏùò\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False  # ÏµúÏ¥àÏóî freeze\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# ÌäúÎãùÎêú Adam Optimizer\n",
    "optimizer = Adam(learning_rate=0.0003, beta_1=0.85, beta_2=0.995)\n",
    "\n",
    "# ÏΩúÎ∞± ÏÑ§Ï†ï\n",
    "checkpoint_path = \"mobilenetv2_adamtune_best.keras\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, verbose=1)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Î™®Îç∏ Ïª¥ÌååÏùº\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ÌïôÏäµ ÏãúÏûë\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpointer, lr_reducer, early_stopper],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ÌïôÏäµ Í≥°ÏÑ† Ï†ÄÏû•\n",
    "fig_path = r\"C:\\Users\\ghwns\\HJ_git\\CV-Projects\\real-time-daily-activity-recognizer\\figures\\exp_20250606_rgb_transfer_mobilenet_adamtune.png\"\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Training / Validation Accuracy (MobileNetV2 + Adam Tuned)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bb120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
